梯度下降法，学习速率

随机梯度下降法 (**SGD**)  ：一次抽取一个样本

小批量梯度下降法：每批包含小批量的样本（随机抽取）。损失和梯度在整批范围内达到平衡



数据集划分：训练集、验证机、测试集![包含三个阶段的工作流程图。

![水平条分为三段：70% 属于训练集，15% 属于验证集，15% 属于测试集](https://developers.google.com/machine-learning/crash-course/images/PartitionThreeSets.svg)![与图 2 类似的工作流程，不同之处在于该工作流程使用验证集而不是测试集来评估模型。然后，在训练集和验证集大致达成一致后，使用测试集确认模型效果。](https://developers.google.com/machine-learning/crash-course/images/WorkflowWithValidationSet.svg)

特征工程：

独热编码：二进制只有一位为1其余为0来表示某一类别



在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

使用one-hot编码，将**离散特征的取值扩展到了欧式空间**，离散特征的某个取值就对应欧式空间的某个点。

https://www.imooc.com/article/35900



好特征：具有非零值，并在我们的数据集中出现至少几次或更多次，若一个特征在具有非零值的情况下出现的非常少，或仅出现几次，应当在预处理步骤中过滤掉

1.特征应具有清晰明确的意义

2. 特征不应使用魔数 

3.特征不应随时间变化（数据的平稳性

4.特征不应采用不理性的离群值



清理数据:

1.缩放，若某特征集只包含一个特征，则缩放几乎没多大收益，如果有多个，则：

​	·帮助梯度下降法更快收敛

​	·帮助避免NaN陷阱

​	·帮助模型为每个特征确定合适的权重

2.处理极端离群值：

3.分箱



**相关矩阵**展现了两两比较的相关性，既包括每个特征与目标特征之间的比较，也包括每个特征与其他特征之间的比较。

在这里，相关性被定义为[皮尔逊相关系数](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient)





特征组合：

一些研究表明，将通过特征交叉乘积获得的线性学习效果与深度网络相结合，kk以实现及强大的建模能力



 **正则化**指的是降低模型的复杂度以减少过拟合 



- 如何定义复杂度（模型）？

- 首选较小的权重

- 偏离将会产生成本

- 可以通过 L2 **正则化**（也称为岭正则化）对这种想法进行编码

- - *复杂度（模型）= 权重的平方和*

  - 减少非常大的权重

  - 对于线性模型：首选比较平缓的斜率

  - 贝叶斯先验概率：

  - - 权重应该以 0 为中心
    - 权重应该呈正态分布

如果模型复杂度是权重的函数，则特征权重的绝对值越高，对模型复杂度的贡献就越大。

我们可以使用 **L2 正则化**公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和：



L2 regularization term=||w||22=w12+w22+...+wn2



模型开发者通过以下方式来调整正则化项的整体影响：用正则化项的值乘以名为 **lambda**（又称为**正则化率**）的标量。也就是说，模型开发者会执行以下运算：



minimize(Loss(Data|Model)+λ complexity(Model))

执行 L2 正则化对模型具有以下影响

- 使权重值接近于 0（但并非正好为 0）

- 使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。

  

在选择 lambda 值时，目标是在简单化和训练数据拟合之间达到适当的平衡：

- 如果您的 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。您的模型将无法从训练数据中获得足够的信息来做出有用的预测。
- 如果您的 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。





逻辑回归:

- 线性逻辑回归极其高效。

- - 超快的训练速度和较短的预测时间。
  - 短模型/宽度模型占用大量 RAM。



线性回归的损失函数是平方损失。逻辑回归的损失函数是**对数损失函数**，定义如下：



LogLoss=∑(x,y)∈D−ylog(y′)−(1−y)log(1−y′)



 对数损失函数的方程式与 [Shannon 信息论中的熵测量](https://wikipedia.org/wiki/Entropy_(information_theory))密切相关。它也是[似然函数](https://wikipedia.org/wiki/Likelihood_function)的负对数（假设“y”属于[伯努利分布](https://wikipedia.org/wiki/Bernoulli_distribution)）。实际上，最大限度地降低损失函数的值会生成最大的似然估计值。 



逻辑回归中的正则化

[正则化](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture)在逻辑回归建模中极其重要。如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：

- L2 正则化。
- 早停法，即，限制训练步数或学习速率。

（我们会在[之后的单元](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/video-lecture)中讨论第三个策略，即 L1 正则化。）

假设您向每个样本分配一个唯一 ID，且将每个 ID 映射到其自己的特征。如果您未指定正则化函数，模型会变得完全过拟合。这是因为模型会尝试促使所有样本的损失达到 0 但始终达不到，从而使每个指示器特征的权重接近正无穷或负无穷。当有大量罕见的特征组合且每个样本中仅一个时，包含特征组合的高维度数据会出现这种情况。

幸运的是，使用 L2 或早停法可以防止出现此类问题。







 ## FTRL 优化算法

高维度线性模型可受益于使用一种基于梯度的优化方法，叫做 FTRL。该算法的优势是针对不同系数以不同方式调整学习速率，如果某些特征很少采用非零值，该算法可能比较实用（也非常适合支持 L1 正则化）。我们可以使用 [FtrlOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer) 来应用 FTRL。





分类：评估分类效果的传统方式是准确率，即预测正确树/总数



系数特征组合可能会大大增加特征空间，可能出现的问题：

模型大小（RAM）可能会变得庞大

噪点系数（导致过拟合）



 L2 正则化可以使权重变小，但是并不能使它们正好为 0.0。 

L0正则化：只会因为存在不为0的权重而惩罚，但是没有凸性，难以优化，NP难



L1 和 L2 正则化。

L2 和 L1 采用不同的方式降低权重：

- L2 会降低权重2。
- L1 会降低 |权重|（绝对值）

因此，L2 和 L1 具有不同的导数：

- L2 的导数为 2 * 权重。
- L1 的导数为 k（一个常数，其值与权重无关）。

您可以将 L2 的导数的作用理解为每次移除权重的 x%。如 [Zeno](https://wikipedia.org/wiki/Zeno's_paradoxes#Dichotomy_paradox) 所知，对于任意数字，即使按每次减去 x% 的幅度执行数十亿次减法计算，最后得出的值也绝不会正好为 0。（Zeno 不太熟悉浮点精度限制，它可能会使结果正好为 0。）总而言之，L2 通常不会使权重变为 0。

您可以将 L1 的导数的作用理解为每次从权重中减去一个常数。不过，由于减去的是绝对值，L1 在 0 处具有不连续性，这会导致与 0 相交的减法结果变为 0。例如，如果减法使权重从 +0.1 变为 -0.2，L1 便会将权重设为 0。就这样，L1 使权重变为 0 了。

L1 正则化 - 减少所有权重的绝对值 - 证明对宽度模型非常有效。





### 神经网络

要对非线性问题进行建模，我们可以直接引入非线性函数。我们可以用非线性函数将每个隐藏层节点像管道一样连接起来。

在下图所示的模型中，在隐藏层 1 中的各个节点的值传递到下一层进行加权求和之前，我们采用一个非线性函数对其进行了转换。这种非线性函数称为激活函数。

 ![除了在两个隐藏层之间添加了一行标为“非线性转换层”的粉色圆圈之外，与上一个图一样。](https://developers.google.com/machine-learning/crash-course/images/activation.svg) 



常见激活函数

以下 **S 型**激活函数将加权和转换为介于 0 和 1 之间的值。



F(x)=11+e−x

曲线图如下：

![S 型函数。](https://developers.google.com/machine-learning/crash-course/images/sigmoid.svg)

**图 7. S 型激活函数。**

相较于 S 型函数等平滑函数，以下**修正线性单元**激活函数（简称为 **ReLU**）的效果通常要好一点，同时还非常易于计算。
$$
F(x)=max(0,x)
$$


ReLU 的优势在于它基于实证发现（可能由 ReLU 驱动），拥有更实用的响应范围。S 型函数的响应性在两端相对较快地减少。

![ReLU 激活函数。](https://developers.google.com/machine-learning/crash-course/images/relu.svg)

**图 8. ReLU 激活函数。**

实际上，所有数学函数均可作为激活函数。假设 σ 表示我们的激活函数（ReLU、S 型函数等等）。因此，网络中节点的值由以下公式指定：
$$
σ(w⋅x+b)
$$
对于神经网络而言，过拟合是一种真正的潜在危险。您可以查看训练数据损失与验证数据损失之间的差值，以帮助判断模型是否有过拟合的趋势。**如果差值开始变大，则通常可以肯定存在过拟合。**



反向传播算法（BP）建立在梯度下降法的基础上

 https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll/ 



反向传播：注意事项

- 梯度很重要

- - 如果它是可微的，则我们也许能够对其进行学习

- 梯度可能会消失

- - 每个额外的层都会依次降低信噪比
  - ReLu 在这里很有用

- 梯度可能会爆炸

- - 学习速率在这里很重要
  - 批标准化（实用按钮）可以提供帮助

- ReLu 层可能会消失

- - 保持冷静，并降低您的学习速率



- 我们希望特征具有合理的范围

- - 大致以 0 为中心，[-1, 1] 的范围通常效果比较好
  - 有助于梯度下降法收敛；避免 NaN 陷阱
  - 避免离群值也会有帮助

- 可以使用一些标准方法：

- - 线性缩放
  - 为最大值和最小值设定硬性上下限（截断）
  - 对数缩放



- 丢弃：另一种正则化形式，对神经网络很有用

- 工作原理是，在一个梯度步长中随机“丢弃”网络的单元

- - 有一个可用于集成学习此处的模型的连接

- 丢弃得越多，正则化效果就越强

- - 0.0 = 无丢弃正则化
  - 1.0 = 丢弃所有内容！学不到任何规律
  - 中间值更有用





# 多类别神经网络 (Multi-Class Neural Networks)

一个典型的方式就是通过一对多的多类别分类



- 多类别单一标签分类：

- - 一个样本可能只是一个类别的成员。
  - 类别互斥这一限制是有用的结构。
  - 有助于在损失中对此进行编码。
  - 将一个 softmax 损失用于所有可能的类别。

- 多类别多标签分类：

- - 一个样本可能是多个类别的成员。
  - 无需对类别成员资格设定额外的限制。
  - 将一个逻辑回归损失用于每个可能的类别。



SoftMax 选项

- **完整 Softmax** 是我们一直以来讨论的 Softmax；也就是说，Softmax 针对每个可能的类别计算概率。
- **候选采样**指 Softmax 针对所有正类别标签计算概率，但仅针对负类别标签的随机样本计算概率。例如，如果我们想要确定某个输入图片是小猎犬还是寻血猎犬图片，则不必针对每个非狗狗样本提供概率。

类别数量较少时，完整 Softmax 代价很小，但随着类别数量的增加，它的代价会变得极其高昂。候选采样可以提高处理具有大量类别的问题的效率。

一个标签与多个标签

Softmax 假设每个样本只是一个类别的成员。但是，一些样本可以同时是多个类别的成员。对于此类示例：

- 您不能使用 Softmax。
- 您必须依赖多个逻辑回归。

例如，假设您的样本是只包含一项内容（一块水果）的图片。Softmax 可以确定该内容是梨、橙子、苹果等的概率。如果您的样本是包含各种各样内容（几碗不同种类的水果）的图片，您必须改用多个逻辑回归。



Softmax 方程式如下所示：

![image-20191123205115496](C:\Users\hasee\AppData\Roaming\Typora\typora-user-images\image-20191123205115496.png)

请注意，此公式本质上是将逻辑回归公式延伸到了多类别。



选择嵌套维度个数

- 嵌套维度的个数越多，越能准确地表示输入值之间的关系

- 不过，维度个数越多，过拟合的可能性就越高，训练速度也会越慢

- 经验法则（一个不错的起点，但应使用验证数据进行微调）：

  ![image-20191123211655038](C:\Users\hasee\AppData\Roaming\Typora\typora-user-images\image-20191123211655038.png)



协同过滤（collaborative filtering）



Word2vec

Word2vec 是 Google 为了训练字词嵌套而研发的一种算法。Word2vec 基于**分布假设**，将语义上相似的字词映射到在几何图形上邻近的嵌套矢量。

分布假设指出经常具有相同相邻字词的字词往往在语义上相似。“狗”和“猫”这两个字词经常靠近“兽医”一词出现，这就可以说明这两个字词在语义上相似。正如语言学家约翰·弗斯 (John Firth) 在 1957 年所言：“观其伴而知其意”。

Word2Vec 通过训练神经网络来区分实际共同出现的多组字词与随机出现在一起的字词，从而充分利用此类上下文信息。输入层采用一种稀疏表示法用于组合一个目标字词与一个或多个上下文字词。这一输入层会连接到一个较小的隐藏层。

在其中一版算法中，系统通过用随机噪点字词替代目标字词来举出反面示例。在给出正面示例“the plane flies”的情况下，系统可能会换成“jogging”来创建对比鲜明的反面示例“the jogging flies”。

另一版算法通过将真实的目标字词与随机选择的上下文字词配对来创建反面示例。因此，系统可能会举出正面示例（(the, plane)、(flies, plane)）和反面示例（(compiled, plane)、(who, plane)），然后通过学习分辨哪几对真正地在文字中一起出现。

不过，分类器不是上述任何一版算法的真正用途。在训练模型后，你得到的是一组嵌套。借助将输入层连接到隐藏层的权重，您可以将字词的稀疏表示映射到小型矢量。这类嵌套可在其他分类器中重复利用。

要详细了解 word2vec，请参阅 [tensorflow.org 上的教程](https://www.tensorflow.org/tutorials/word2vec/index.html)



从广义上讲，训练模型的方式有两种：

- **静态模型**采用离线训练方式。也就是说，我们只训练模型一次，然后使用训练后的模型一段时间。
- **动态模型**采用在线训练方式。也就是说，数据会不断进入系统，我们通过不断地更新系统将这些数据整合到模型中。



**离线**推理

- 使用 MapReduce 或类似方法批量进行所有可能的预测。
- 记录到表格中，然后提供给缓存/查询表。
- 优点：不需要过多担心推理成本。
- 优点：可以使用批量方法。
- 优点：可以在推送之前对数据预测执行后期验证。
- 缺点：只能对我们知晓的数据进行预测，不适用于存在长尾的情况。
- 缺点：更新可能延迟数小时或数天。

**在线**推理

- 使用服务器根据需要进行预测。
- 优点：可在新项目加入时对其进行预测，非常适合存在长尾的情况。
- 缺点：计算量非常大，对延迟较为敏感，可能会限制模型的复杂度。
- 缺点：监控需求更多。



### 公平性：偏差类型

