偏差 (bias)



距离原点的截距或偏移。偏差（也称为**偏差项**）在机器学习模型中用 b 或 w0 表示。例如，在下面的公式中，偏差为 b：


$$
y′=b+w1x1+w2x2+…wnxn
$$
请勿与[**预测偏差**](https://developers.google.com/machine-learning/glossary#prediction_bias)混淆。

预测偏差 (prediction bias)



一种值，用于表明[**预测**](https://developers.google.com/machine-learning/glossary#prediction)平均值与数据集中[**标签**](https://developers.google.com/machine-learning/glossary#label)的平均值相差有多大。





分箱 (binning)/分桶 (bucketing)



将一个特征（通常是[**连续**](https://developers.google.com/machine-learning/glossary#continuous_feature)特征）转换成多个二元特征（称为桶或箱），通常根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。





协同过滤 (collaborative filtering)



根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。



卷积 (convolution)



简单来说，卷积在数学中指两个函数的组合。在机器学习中，卷积结合使用卷积过滤器和输入矩阵来训练权重。

机器学习中的“卷积”一词通常是[**卷积运算**](https://developers.google.com/machine-learning/glossary#convolutional_operation)或[**卷积层**](https://developers.google.com/machine-learning/glossary#convolutional_layer)的简称。

如果没有卷积，机器学习算法就需要学习大张量中每个单元格各自的权重。例如，用 2K x 2K 图像训练的机器学习算法将被迫找出 400 万个单独的权重。而使用卷积，机器学习算法只需在[**卷积过滤器**](https://developers.google.com/machine-learning/glossary#convolutional_filter)中找出每个单元格的权重，大大减少了训练模型所需的内存。在应用卷积过滤器后，它只需跨单元格进行复制，每个单元格都会与过滤器相乘。



嵌套 (embeddings)



一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：

- 表示成包含百万个元素（高维度）的[**稀疏向量**](https://developers.google.com/machine-learning/glossary#sparse_features)，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。
- 表示成包含数百个元素（低维度）的[**密集向量**](https://developers.google.com/machine-learning/glossary#dense_feature)，其中每个元素都存储一个介于 0 到 1 之间的浮点值。这就是一种嵌套。

在 TensorFlow 中，会按[**反向传播**](https://developers.google.com/machine-learning/glossary#backpropagation)[**损失**](https://developers.google.com/machine-learning/glossary#loss)训练嵌套，和训练[**神经网络**](https://developers.google.com/machine-learning/glossary#neural_network)中的任何其他参数一样。





集成学习 (ensemble)



多个[**模型**](https://developers.google.com/machine-learning/glossary#model)的预测结果的并集。您可以通过以下一项或多项来创建集成学习：

- 不同的初始化
- 不同的[**超参数**](https://developers.google.com/machine-learning/glossary#hyperparameter)
- 不同的整体结构

[深度模型和宽度模型](https://www.tensorflow.org/tutorials/wide_and_deep)属于一种集成学习。



假负例 (FN, false negative)



被模型错误地预测为[**负类别**](https://developers.google.com/machine-learning/glossary#negative_class)的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。



假正例 (FP, false positive)



被模型错误地预测为[**正类别**](https://developers.google.com/machine-learning/glossary#positive_class)的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。



假正例率（false positive rate, 简称 FP 率）



正例率（true positive rate, 简称 TP 率）



与[**召回率**](https://developers.google.com/machine-learning/glossary#recall)的含义相同，即：



![image-20191124172131085](C:\Users\hasee\AppData\Roaming\Typora\typora-user-images\image-20191124172131085.png)

正例率是 [**ROC 曲线**](https://developers.google.com/machine-learning/glossary#ROC)的 y 轴。



[**ROC 曲线**](https://developers.google.com/machine-learning/glossary#ROC)中的 x 轴。FP 率的定义如下：



![image-20191124165100706](C:\Users\hasee\AppData\Roaming\Typora\typora-user-images\image-20191124165100706.png)

超参数 (hyperparameter)



在模型训练的连续过程中，您调节的“旋钮”。例如，[**学习速率**](https://developers.google.com/machine-learning/glossary#learning_rate)就是一种超参数。

与[**参数**](https://developers.google.com/machine-learning/glossary#parameter)相对。



L1 损失函数 (L₁ loss)



一种[**损失**](https://developers.google.com/machine-learning/glossary#loss)函数，基于模型预测的值与[**标签**](https://developers.google.com/machine-learning/glossary#label)的实际值之差的绝对值。与 [**L2 损失函数**](https://developers.google.com/machine-learning/glossary#squared_loss)相比，L1 损失函数对离群值的敏感性弱一些。



L1 正则化 (L₁ regularization)



一种[**正则化**](https://developers.google.com/machine-learning/glossary#regularization)，根据权重的绝对值的总和来惩罚权重。在依赖[**稀疏特征**](https://developers.google.com/machine-learning/glossary#sparse_features)的模型中，L1 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 [**L2 正则化**](https://developers.google.com/machine-learning/glossary#L2_regularization)相对。



L2 损失函数 (L₂ loss)



请参阅[**平方损失函数**](https://developers.google.com/machine-learning/glossary#squared_loss)。



L2 正则化 (L₂ regularization)



一种[**正则化**](https://developers.google.com/machine-learning/glossary#regularization)，根据权重的平方和来惩罚权重。L2 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 [**L1 正则化**](https://developers.google.com/machine-learning/glossary#L1_regularization)相对。）在线性模型中，L2 正则化始终可以改进泛化。



标准化 (normalization)



将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。



离群值 (outlier)



与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。

- 绝对值很高的[**权重**](https://developers.google.com/machine-learning/glossary#weight)。
- 与实际值相差很大的预测值。
- 值比平均值高大约 3 个标准偏差的输入数据。

离群值常常会导致模型训练出现问题。



先验理念 (prior belief)



在开始采用相应数据进行训练之前，您对这些数据抱有的理念。例如，[**L2 正则化**](https://developers.google.com/machine-learning/glossary#L2_regularization)依赖的先验信念是[**权重**](https://developers.google.com/machine-learning/glossary#weight)应该很小且应以 0 为中心呈正态分布。

